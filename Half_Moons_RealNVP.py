# -*- coding: utf-8 -*-
"""PFE_2D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L_SH5uVje_jxLvMbh2XvBmm8lCB2kZha

# Normalizing flow |
"""



"""# Real-NVP | 2D | Half Moons

Données
"""

import torch
import torch.utils.data as data 
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from torch import distributions
from sklearn.datasets import make_moons

class NumpyDataset(data.Dataset):
    def __init__(self, array):
        super().__init__()
        self.array = array

    def __len__(self):
        return len(self.array)

    def __getitem__(self, index):
        return self.array[index]


n_train, n_test = 10000, 5000 #10000 à changer, peut être augmenter le réseau également
train_data, train_labels = make_moons(n_samples=n_train, noise=0.1)
test_data, test_labels = make_moons(n_samples=n_test, noise=0.1)

train_loader = data.DataLoader(NumpyDataset(train_data), batch_size=128, shuffle=True)
test_loader = data.DataLoader(NumpyDataset(test_data), batch_size=128, shuffle=True)

plt.plot(train_data[:, 0][train_labels==1], train_data[:, 1][train_labels==1], "bs")
plt.plot(train_data[:, 0][train_labels==0], train_data[:, 1][train_labels==0], "g^")

#Réseau de neurones
class SimpleMLP(nn.Module):
    def __init__(self, input_size, hidden_size, num_hidden_layers, output_size):
        super(SimpleMLP, self).__init__()
        layers = [nn.Linear(input_size, hidden_size)]
        layers.append( nn.ReLU() )
        for _ in range(num_hidden_layers - 1):
            layers.append( nn.Linear(hidden_size, hidden_size) )
            layers.append( nn.ReLU() )
        layers.append( nn.Linear(hidden_size, output_size) )
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)

#NVP
class RealNVP(nn.Module):
    def __init__(self, mlp, mask, target_distribution):
        super(RealNVP, self).__init__()
        self.target_distribution = target_distribution
        self.mask = mask
        self.mlp = mlp
        #self.t = torch.nn.ModuleList([mlp])
        #self.s = torch.nn.ModuleList([mlp])
        self.scale_scale = nn.Parameter(torch.zeros(1), requires_grad=True)
        self.shift_scale = nn.Parameter(torch.zeros(1), requires_grad=True)
        self.scale_scale_t = nn.Parameter(torch.zeros(1), requires_grad=True)
        self.shift_scale_t = nn.Parameter(torch.zeros(1), requires_grad=True)
        
    def g(self, z):
        x = z[:,0]
        
        for i in range(len(self.mask)):
          #x de la forme [x1 , y1], on souhaite avoir que x1 pour le premier layer donc mask = [1,0]
          x_masked = x*self.mask[i]

          #On prend les log_gamme et beta
          t = mlp(x_masked) * (1-mask[i])
          log_s = mlp(x_masked) * (1-mask[i])
          log_s = log_s.tanh() * self.scale_scale + self.shift_scale
          
          #log_s = mlp(z_masked) * (1-mask[i])
          #t = mlp(z_masked) * (1-mask[i])
          #log_s = log_s.tanh() * self.scale_scale 
          
          #x = x * torch.exp(log_s) + t
          x = x_masked + (1 - self.mask[i]) * (x * torch.exp(log_s) + t)
        return x

    def f(self, x):
        log_det_J, z = x.new_zeros(x.shape[0]), x

        for i in reversed(range(len(self.mask))):
          z_masked = self.mask[i] * z

          #log_s, t = self.mlp(z_masked).chunk(2, dim=1)
          #log_s = mlp(z_masked) * (1-mask[i])
          #t = mlp(z_masked) * (1-mask[i])
          #log_s = log_s.tanh() * self.scale_scale 

          t = mlp(z_masked) * (1-mask[i])
          log_s = mlp(z_masked) * (1-mask[i])
          log_s = log_s.tanh() * self.scale_scale + self.shift_scale
          
          z = (1 - self.mask[i]) * (z - t) * torch.exp(-log_s) + z_masked
          #z = (z - t) * torch.exp(-log_s)
          log_det_J -= log_s.sum(dim=1)

        return z, log_det_J
    
    def log_prob(self,x):
        z, logp = self.f(x)
        return self.target_distribution.log_prob(z) + logp

    def sample(self, batchSize): 
        z = self.target_distribution.sample((batchSize, 1))
        logp = self.target_distribution.log_prob(z)
        x = self.g(z)
        return x

mlp = SimpleMLP(2,512,5,2)
mask = nn.Parameter(torch.from_numpy(np.array([[0, 1], [1, 0], [0, 1], [1, 0]]).astype(np.float32)), requires_grad=False)
prior_z = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))
flow = RealNVP(mlp, mask, prior_z)

optimizer = torch.optim.Adam(flow.parameters(), lr=1e-4)
def train(flow, optimizer, train_loader, num_epochs):
  train_losses, test_losses = [], []
  for epoch in range(num_epochs):
    for x in train_loader:
      optimizer.zero_grad()
      loss = - flow.log_prob(x.float()).mean()  
      loss.backward(retain_graph=True)
      optimizer.step()
    train_losses.append(loss)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")
  return train_losses

train_losses = train(flow, optimizer, train_loader, 70)

losses = []
for i in range(len(train_losses)):
  losses.append(train_losses[i].detach().numpy())

_ = plt.plot(losses, label='train_loss')
#_ = plt.plot(test_losses, label='test_loss')
plt.legend()

def plot_moons(ax, data, title):
    X = data
    ax.scatter(X[:, 0], X[:, 1], c='r')
    ax.set_title(title)
    ax.set_xticks([])
    ax.set_yticks([])

# Define figure and axes
fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))
fig.tight_layout()

# Plot original data
plot_moons(axs[0, 0], test_data, r'$X \sim p(X)$')

# Plot prior distribution
z = prior_z.sample((1000, 1))
axs[0, 1].scatter(z[:, 0, 0], z[:, 0, 1])
axs[0, 1].set_title(r'$Z \sim p(z)$')
axs[0, 1].set_xticks([])
axs[0, 1].set_yticks([])

# Plot transformed data
x = []
for _ in range(1):
    x.append(flow.sample(2000).detach().numpy())
x = np.concatenate(x)
plot_moons(axs[1, 0], x, r'$X = g(z)$')

# Plot learned distribution
z = []
for x in test_loader:
    z.append(flow.f(x.float())[0].detach().numpy())
z = np.concatenate(z)
axs[1, 1].scatter(z[:, 0], z[:, 1])
axs[1, 1].set_title(r'$Z = f(X)$')
axs[1, 1].set_xticks([])
axs[1, 1].set_yticks([])

# Show plot
plt.show()